The best solutions for all particle positions  are also updated according to the fitness evolution matrix.
PSO for our AE search problem.
We choose different neural network architectures including Mlp-mixer , Swim-transformer  and ResNet  to test the effectiveness of our method on the non-defensive models on ImageNet with Theta norm.
According to the procedure of GAQA, the time overhead consists of eight parts during the GA search framework.
First, the solutions in the current generations are sorted by the decrease order of their fitness values.
However, keeping certain weak solutions into the next iteration is also absolutely necessary since they also have certain superior genes in their chromosomes.
As a flexible searching framework, GA can be designed to support both single-objective and multi-objective optimization problems.
Otherwise, PSO-Jump continues the evolving procedure by updating Theta  and  Theta by Theta .
Therefore, this operator helps GAQA to find the adversarial examples quickly.
We evaluate the effectiveness of the proposed method on defensive models and non-defensive models in Section V and Section V.
Attack Successful Ratio : the ratio of incorrect classification in the input images.
ASR, MQ and AQ comparisons on different Theta norm.
Defenders  could use image compression to reduce high-dimensional data to low-dimensional data.
Theta is the perturbation bound of Theta-norm.
For the situation of failed AE , Theta is less than zero.
This is because the white-box attacks have the gradient information to modify the perturbation direction whereas our method explores the direction.
The specific values of Theta and block size will be adaptively changed during each iteration, thus improving the convergence of QAGA.
First, the query efficiency of GAQA is much higher than whitebox attacks, although this is reasonable because of the zeroknowledge of victim models.
Consequently, PSO-Jump will take the time overhead of Theta.
Normalized Euclidean Distance : the difference between the original images and the adversarial images.
However, these methods are query-intensive and computation-expensive, thus limiting their applications in real-world applications.
In addition, we use the 0-1 matrix Theta to represent the gene-shuffling selection of each selected father chromosome, where each element Theta of Theta  is set to 1 with the probability of Theta.
Theta.
In other words, the objective of attack evasiveness needs to be improved only if the AE attack is effective.
Fixing Data Augmentation   on CIFAR-100: using generated examples to provide a greater diversity of augmentations that help enhance the image manifolds and allow adversarial training.
Since GAQA can generate better adversarial examples according to equation.
Our method has average Theta, Theta, Theta, Theta and Theta reduction in NED, respectively, compared with APGD-CE, ZOO, Square, DeepFool and GenAttack.
Theta, Theta or Theta.
In natural selection, the superior offspring may be born with elite fathers and inferior fathers.
Note that, each element of the matrix chromosome Theta can change independently.
Due to high time overhead to find the optimal Theta for each solution Theta, we choose to search for the near-optimal Theta only for Theta in each generation, thus reducing the complexity of the query process.
ASR, MQ and AQ comparisons on different nerual network architectures.
In general, evolutionary operators of selection, crossover and mutation are utilized to breed the next-generation population.
Theta is related to the speed Theta in 3rd generation, the best-so-far perturbation for the first particle , and the best perturbation in the 3rd generation .
The Theta-th solution in Theta-th generation  with higher fitness Theta function values are more likely to be selected in the next generation Theta.
As a supplementary work, this paper approaches to implement black-box AE attacks on defensive DNN models deployed in servers like MLaaS.
This paper will focus on dealing with IT-based detection and defense within MLaaS.
For any point Theta, it satisfies equation.
First, GAQA takes Theta to initialize the perturbation population.
But recently some paper like graphMAE  can consistently generate outperformance over both contrastive and generative state-of-the-art baselines.
Based on the predicted Theta, Theta can be calculated by the following equation.
Graph Embedding Models  ,  elaborates the expressive power of deep learning on graph-structured data, have achieved remarkable success in various domains, such as drug discovery ,  social network analysis , , computer version , medical imaging , financial surveillance , structural role classification  and automated machine learning .
Our approach is attacking a defensive model.
We assume Theta as the Theta-th solution in Theta-th generation and  Theta as the Theta-th solution in Theta-th generation.
Benchmark-based experiments on CIFAR-10, CIFAR-100 and ImageNet-1K are conducted to evaluate the efficiency of our method in both defensive and non-defensive scenarios.
Similar to card shuffling in card-play games, we randomly select the genes from their fathers to generate new solutions.
The ASRs of all three types have no overlap, which may be because the bigger scale datasets have the larger search spaces.
In the attack epoch, SAT will calculate feature weight firstly.
GAQA is designed to pick up certain top perturbations  for reassembling operations.
PSO-Jump will terminate the PSO evolving when generating one effective AE.
Such as poison and evasion attack, black-box and white-box attack.
Besides the objective of attack effectiveness, we need to evaluate the attack coverage ratio of the proposed black-box attack method.
Given the input image Theta having the width Theta and the height Theta, Theta the MCSD can be obtained by equation.
To improve the convergence, GAQA conducts the adaptation operation to update the reproduction-related parameters according to equations.
To obtain the fitness value , we need to sort the confidence values of Theta labels .
Average Queries : the average query times in the three experiments.
We have conducted to extensive experiments to evaluate the efficiency of our method.
Then we need to calculate the probability of being chosen, for an edge array, we choose Theta edges which haven't appeared in the array to replace the array element randomly.
Theta, Theta and Theta.
The six attack methods can be divided into three types: GAQA, Genattack and others.
To sum up, removing any operator will lead to the drop of ASR in the non-defensive or defensive models, which demonstrates that each operator of GAQA plays a role in the searching process.
However, it is not suitable for the search framework of GAQA, because we always expect to keep the perturbations better fitness values even with mutation operations.
Since this paper only considers to generate adversarial examples for images of general size, it would be a challenging work to extend our searching framework to support the generation of adversarial examples for super large images.
It would be good policy to produce new offspring  in the direction of perturbation-direction of input data.
Then use the chain derivation to get feature weight.
PSO is designed to an iterative searching algorithm by imitating the food-finding process of a flock of birds.
Crossover operation is used to mix the genes from two different perturbations in order to generate the greater perturbation with lower loss.
Due to the bigger search space of the large-size images, our method needs more queries to estimate the evolving direction.
Next we will pass the feature and feature weight to SAT to get loss.
Specifically,  Theta can be calculated according to equation.
In other words, the requirement satisfied adversarial example can not be generated even with high time overhead.
Then, we assign a filter probability for each solution.
Let Theta, searching with multi-granularity in the direction of Theta can help to find the optimal solution.
DNNs are designed to learn the high-level features using layer-by-layer data processing in high-dimension space, similar to the information flow of neurons in living organisms.
Lastly, we define the problem shown as equation.
Given Theta as the minimal value of attack effectiveness , the fitness function is transformed to optimize the attack evasiveness while guaranteeing the attack is effective.
In addition, most of existing AE attacks including both white-box and black-box methods ignore to generate AEs for defensive DNNs in the scenarios of MLaaS.
To deceive the perception of human eyes, each element of the perturbation has the bound of Theta.
IT is devised to reduce the model sensitivity to perturbations on original images.
Theta is the dimension of input Theta, Theta is the bound of Theta-norm to escape human perception.
The first part is related to the objective of attack effectiveness, whereas the second part focuses on the objective of attack evasiveness.
So we proposed feature sensitivity to enhance SAT's Effective.
This is reasonable since white-box attacks need to know the detail information of model architecture, model parameters and training data of the victim DNN models.
FL and RAT are based on ImageNet.
With the purpose of preventing the disappearance of elite solutions and ensuring the non-decrease of the fitness function, the solutions with high fitness values are generally expected to be kept in the next generation.
Then, this paper proposes a query-efficient AE attack based on Genetic Algorithm  and Particle Swarm Optimization  to address the perturbation optimization problem.
In this section, extensive experiments are conducted to evaluate the effectiveness of the proposed GAQA attacks son DNN models.
Most of the graph embedding models nowadays are trained with Self-supervised learning , which can be generally categorized into generative and contrastive methods , has found widespread adoption in computer vision  and natural language processing .
Deep Neural Networks  have been immensely deployed into artificial intelligence-driven applications , ranging from identification recognition to environment surveillance and to smart robot applications.
We propose a GA-based query-efficient attack method to generate image AEs to breakthrough victim models with defensive mechanisms.
In Section VI , extensive experiments are conducted to evaluate the performance of proposed method.
Well-designed evolutionary algorithms have been used for image generation , image classification   and neural architecture search .
Removing the PSO-Jump will increase MQ and AQ.
Theta and model classify it into Theta, which is called positive examples.
For convenience we define edge perturbation Theta and node perturbation Theta.
The equation is shown as equation.
Adversarial training is an useful tool to build a stronger DNN, where defenders could use attack method to generate adversarial examples and put them into training process.
We define Theta as mean convolution standard deviation to evaluate the high frequency signals of AE, which calculates the standard deviation of a pixel point and its eight adjacent points.
The Theta function will return the variance for each row for a two-dimension matrix.
We choose Mlp-mixer , Swim-transformer  and ResNet  as the non-defensive models.
Therefore, we can conclude that the PSO-Jump of our GAQA is effective.
Genattack has the smallest CR among the six attack methods, which may be because it has no adaptive function and well-designed reproduction process.
The original training dataset is unknown to defenders, meaning that the defense strategy cannot be designed from the perceptive of the dataset.
For instance, the produce of Theta depends on the solution Theta in the position Theta of 3rd generation and the speed Theta according to equation.
For the second question, we cut the edge array into Theta fragments and splicing them together is like genetic recombination.
Compared with the black-box methods, our method has Theta increase in ASR.
Given the benign input Theta with the class label as Theta, the perturbed Theta is defined as an AE if misclassified to the label Theta.
UD, WDM and ODEs are based on CIFAR10.
Based on the evolution matrix for the first three generations, the solutions of 4th generation can be produced.
AE generation.
Theta is decreasing with the growing of Theta.
Theta determine the granularity of perturbation discretization, and it will be adaptively changed in the search iteration.
Given a clean image, the corresponding AE depends on the perturbation on each pixel of the image.
Comparing with white-box attacks, they are more practical since they are assumed to be zero-knowledge of model architecture, parameters and the training data.
Then SAT will update the node perturbation and edge perturbation respectively.
Particle Swarm Optimization  can obtain optimized solutions in a large stochastic searching space by imitating the food-finding process of a swarm of birds .
Then PSO-Jump calculates the perturbation of Theta according to  Theta and Theta  .
For comparison, we valuate each operator of GAQA on the non-defensive model  and the defensive model .
Secondly, those dirty graphs will be pass to the victim model and valid model to get embedding result.
Especially for defensive models, our methodcan obtain top attack performance with the lowest query costcompared to black-box attacks.
GM is also cost-expensive and not suitable for black-box attacks.
We start our experiments on the CIFAR-10 dataset with Theta norm.
Considering the linear characteristics of DNNs, FGSM  is proposed to generate AEs using on-step gradient updation on image pixels.
Mean Convolution Standard Deviation : the smoothness of a adversarial image.
In other words, the predicted Theta depends on the so-far optimal fitness and the number of iterations.
Our method has more MQ and AQ on the large-size datasets.
Section II describes related work.
The solution set of this  problem contains many locally optimal solutions.
Revisiting Adversarial Training   on ImageNet: using a weaker and cheaper adversary by FGSM to train a defensive model with higher robust accuracy.
With the purpose of alleviating the vulnerability to AE attacks, certain mechanisms are emerging to improve the defensive performance of DNNs including AE detection and recognition , adversarial training of DNNs  and image compression  in the image domain.
After that, GAQA will conduct the superiority-related probabilistic selection on the current perturbation solutions , wherein the inferior solutions will be deleted from the current generation .
Next, SAT will initial the perturbation and start attack loops.
In case of non-defensive models, our method can successfully attack state-of-the-art network architectures including Mlp-mixper and Swim-Transformer.
The query operation of the victim model can be defined as the function Theta, within which Theta returns the confidence of recognizing input Theta as class label Theta.
In this equation, Theta is the model weight of encoder.
The adversaries have the right of accessing to DNNs, and mostly exploit back-propagation methods to obtain the gradient information of DNN models, thus generating white-box AEs to threaten the victim models.
We use CIFAR-10, CIFAR-100 and ImageNet to test evaluate the effectiveness of our method on ResNet with Theta norm.
We can observe that Theta will exceed the Theta according to equation.
Another way to defend against attack is to construct a robust model .
The better Theta of offspring will be remained after superiority-driven probabilistic selection.
This is a high-dimension coordinate system in input space.
STATE Calculate Theta loss according equation.
We use three dataset including CIFAR-10, CIFAR-100 and ImageNet to evaluate the effectiveness of the proposed method on defensive models and non-defensive models.
PSO-Jump is checked whether to be invoked by comparing Theta to the predefined threshold Theta .
During the searching iteration, the parameters related reproduction operators need to be updated.
Adaptive mutation is designed to improve the diversity of the perturbations in Stage .
Theta must be limited.
We expect to initiate the perturbations of all solutions to fill the searching space uniformly.
Moreover, removing the initialize-population will increase MQ by Theta with a Theta drop in ASR.
When the estimated steps exceed the threshold, the iterative process of PSO will be performed.
PSO-Jump, which takes a set of perturbation particles  as the input.
Defenders  usually use white-box attack methods to generate adversarial examples because they have access to the parameters and structure of their model.
In generative embedding model, we noticed sometimes the embedding result of each node has a relatively close proximity, which may make it difficult for downstream task models to distinguish between these nodes, resulting in performance degradation.
The construction is shown as equation.
Specifically, the calculation of Theta can be referred to equation.
Low Temperature Distillation   on CIFAR-100: using the knowledge distillation framework to generate the desired soft labels.
In Stage , new solutions  will be generated via Reproduction operators including Perturbation-varied reproduction, Block-based crossover, and Shuffle-based crossover.
Second, ablation study is presented to demonstrate the efficiency of each step of our GAQA in Section V.
SAT use both gradient descent and genetic algorithms to update the node perturbations and edge perturbations.
Our method achieves a Theta increase in ASR and respectively decreases AQ and MQ by Theta and Theta compared with the three black-box methods.
First, the number of discrete perturbations, Theta, needs to be refined.
Besides, the vast searching will decrease the efficiency of out proposed SAT.
The two white-box attack methods  and our method can achieve Theta ASR.
A Defensive DNN model is defined as Theta, which is able to alleviate the impact of perturbed input Theta with the defensive policy Theta.
Its success has been largely built upon relatively complicated training strategies.
Although we have introduced certain mechanisms to avoid the searching into local optimal space, there still has the possibility to fall into the local optimum after a large number of evolving iterations.
The crossover and mutation operator are designed to increase the diversity of population while the selection operator is used to find globally better solutions Theta in the search space.
For example,  Sun  use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize.
The average MCSD of GAQA is  Theta, Theta, Theta, Theta and Theta of those of APGD-CE, ZOO, Square, DeepFool, and GenAttack, respectively.
It is of critical importance to construct the initial population Theta with the purpose of improving the search efficiency.
According to the pseudo code of PSO-Jump, the main time overhead depends on the Theta iterations to calculate the speed Theta and the perturbation Theta for particle Theta, which takes time overhead of Theta.
Theta represents the existing attack methods.
ASR.
The mutation probability Theta and shuffling probability Theta also need to be updated with the predicted Theta.
This paper exploits a group of Theta to reproduce a batch of perturbations.
This could guarantee all genes of Theta are independently inherited from fathers.
But the node update may exceed the limits, we need to revise it to satisfy the attack limits.
Following that, BIM   is further designed to generate better AEs by iteratively using FGSM.
Less MCSD means that our adversarial examples have less high frequency signals, which makes our method unlikely to be detected by identification modules or cleared by the compression modules.
As one popular method, Genetic Algorithm  is popular to search optimal solutions for constrained optimization problems.
The attack process could be divided into five steps.
Meanwhile, our approach has the same ASR with the white-box attack methods while our method has lower queries compared with other black-box attack methods.
SAT is designed to iteratively evolve the perturbations clean graph, with the purpose to successfully attack the graph embedding model.
Meta attack, Dice attack, Random attack , embed attack.
Query stage, the adversarial examples  are feed to the victim model to get the corresponding confidence values of prediction classes.
We assume to predict the remaining iterations when GAQA has searched for 100 iterations.
The hyper-parameters of SAT are given in TABLE I.
Therefore, our method has greater ASR and lower MQ on different neural network architectures.
Similarly, we consider to choose the solutions in elite population and inferior population to implement the shuffle-based crossover.
APGD-CE , ZOO , Square-Attack , DeepFool , GenAttack .
Therefore, we only need to optimize the second part to find perturbations with high attack evasiveness.
Afterwards, PSO-Jump will evaluate the fitness value Theta, and check whether it is greater than Theta .
So in SAT, we use equation.
Although Genattack is a genetic algorithm, it has poor performance.
Median Queries :  the median query times in the three experiments.
The three black-box attack methods  and our method have higher MQ and AQ, since they need to estimate the evolution direction through the query model.
The fragment in the same position in Theta comes from a different parent array than Theta.
We use it to construct dirty edge Theta and dirty node attribute Theta by the equation.
Some existing attacks  consider to restart the searching by dropping all the perturbations, which is waste of the searching efforts.
However, due to the weak interpretability, DNNs has the inherent vulnerability to Adversarial Example  attacks  .
In this paper, we aim to generate a poisoned training dataset which will decrease the performance of graph embedding.
A lower loss can indicate that the  embedding model's performance on a downstream task is still poor even if it is trained well enough.
GAQA can reduce Theta, Theta, Theta, Theta and Theta in NED, respectively, compared with APGD-CE, ZOO, Square, DeepFool, and GenAttack in three defensive models.
Those processes are shown in equation.
Similarly, the evolutions of Theta, Theta and Theta for the 4th generation can be conducted.
To sum up, removing any operator will lead to the drop of ASR in both non-defensive and defensive models.
At the same time, we make edges with higher edge weights more likely to be selected.
Since the non-defensive models have many weaknesses, attack methods have fewer queries compared with attacking the defensive models.
Meanwhile, it attracted researchers use attack approaches to test the robustness of it  .
Sort Theta by the decrease order of fitness.
We assume the distance between any two consecutive perturbations are the same.
Theta is probability of the edge  being chosen.
Then  we define an area Theta  named convex space around Theta in high-dimension space, it satisfies equation.
Black-box AE attacks are also seriously threatened to DNNs .
More specific, this paper aims to deploy a query-based gradient-free black-box attack on MLaaS, which can adaptively upgrade the attack strength of adversarial inputs according to the returned confidence values of all classes.
Ordinary Differential equationuations   on CIFAR-10: imposing constraints to ensure that all eigenvalues of the Jacobian matrix of the neural ODE layer have negative real parts, when each classification class converges to its own equilibrium points.
The effectiveness of attack methods is evaluated by seven metrics.
Along with the fast development of AE attacks, the robustness of DNNs has received increasing concerns.
Node attribute Theta and truth label which could be predicted by any downstream task model Theta.
Adversarial example recognition relies on two networks: identification and elimination  model.
Illustration of PSO-based perturbations evolution.
For each edge, we need to estimate its weight and determine the probability that it  be deleted or added.
Therefore, it is practical to set a threshold  for the intolerable searching iterations.
Initiate the perturbation solutions Theta via equation.
TLM  and RAT  as the victim models.
SAT get the feature and use it to train the downstream task model.
CR results on the two defensive models.
This section introduces the preliminaries.
ResNet is a widely used neural network, which contains identity short-cut connections.
Theta and Theta.
To guide the searching direction of GAQA, a fitness function should be designed to evaluate the quality of each solution.
Swim-transformer is a new neural network based on the NLP transformer.
In contrast to white-box attack, block-box attack has a wider search space, because it has no knowledge of the specific distribution of the high-dimensional space around the original example.
Unlabeled Data   on CIFAR-10: using unlabeled data to bridge the sample complexity gap between standard and robust classification.
On the contrary, black-box attacks are assumed to be zero-knowledge of architecture, parameters or any other settings of targeted DNN models.
After formulating the poison attack as a multi-objective optimization problem, which is a non-convex, and a discrete optimization problem, finding the optimal poisoned examples to attack embedding models is extremely difficult due to the explosive search space and potential gradient traps.
Shanqing Yu   proposed a genetic algorithm-based Euclidean distance attack strategy  to attack the network embedding model.
Therefore, our approach has stronger searching capacity and higher ASR.
Similar to GA's searching framework, GAQA is designed to iteratively evolve the perturbations on clean images, with the purpose to successfully attack the defensive victim DNN model.
We chose UD , WDM , ODEs  as the victim neural networks.
The gradient descent algorithm is highly efficient for solving convex problems, but at the same time, it is easy to fall into local optimal solutions when solving non-convex problems, and it is necessary to initialize the start of the search several times to solve a better solution.
GAQA initializes the perturbation population and search-related parameters .
Section presents the threat model and formulate the design to a multi-objective optimization problem.
However, our GAQA has fewer queries compared with the three black-box attack methods.
Training high-dimensional data require larger models.
Theta indicates the standard deviation of Theta and its eight adjacent points.
Therefore, it is important for defenders to find  adversarial examples as much as possible.
Theta represents the maximum percentage of change of each node, for example, if Theta equals to Theta, it means the node attributes won't change by more than Theta.
GAQA has achieved a Theta increase in ASR, and respectively decreased AQ and MQ by Theta and Theta, compared with the three black-box methods .
The smaller NED means that the adversarial examples generated by our method are more similar to the original images.
Section II describes preliminaries work.
Theta is the best solution  of all solutions in the Theta-th iteration.
Assume that the defenders know the model structure and can deconstruct the DNN model to get the weight parameters.
Our method has average Theta, Theta, Theta, Theta and Theta reduction in NED, respectively, compared with APGD-CE, ZOO, Square, DeepFool, and GenAttack.
Note that PSO-Jump is described in the following paragraph.
The average MCSD of GAQA is Theta, Theta, Theta, Theta and Theta of those of APGD-CE, ZOO, Square, DeepFool, and GenAttack, which means that our GAQA has a great evasiveness on the big-scale datasets.
This section conducts ablation experiments to test the effectiveness of our method.
Adversarial Training , Gradient Masking  and Input Transformation  and Theta.
The values of  Theta and Theta will increase with the number of Theta growing.
ASR, MQ and AQ comparisons on different datasets.
Theta is the maximal fitness value for all solutions according to equation.
As a supplementary work, this paper proposes a sensitivity-aware attack framework in order to target attacks on graph embedded models that use structures such as Graph Attention Network , graph convolutional Network  as neural network components.
In this paper, we have made efforts to generate adversarial examples for defensive DNN models deployed in the image-related intelligent systems.
Theta decrease in the number of queries.
In this paper, we propose the elites-preferred probabilistic selection to choose solutions for the coming iterations in our GAQA.
Mlp-mixer is a traditional neural network, which consists of an input layer, several hidden layers and an output layer.
For the situation of successful AE attack, the value of the first part is fixed to  Theta according to the Theta operation on it.
For the evolving procedure, PSO-Jump will update the position of each particle for Theta times .
Since feature-squeezing and image-compress are two most widely used input transformation policies, we must try our best to guarantee the generated AEs can stealthy pass the corresponding process.
Considering a complex DNN Theta  with numerous parameters, it has a powerful  ability to fit the input signal.
Theta is calculated by the following equation.
LTD and FDA are based on CIFAR100.
The Theta function  is to evaluate the performance of  downstream tasks model Theta, Theta is the true label of graph Theta in downstream tasks.
To sum up, our method achieves good performance on the defensive models according to the experimental results on CIFAR-10, CIFAR-100 and ImageNet.
In this section, we valuate the effectiveness of our method on the non-defensive models from three perspectives: neural network architecture, Theta norm and dataset.
For simplicity, we calculate them with the same following equation.
The compression phase Theta will compress an input image.
Jumping process  will further used to generate non-local optimal solutions when the number of iterations reaches at the predefined threshold Theta.
Theta is designed to decrease with the increase of Theta to guarantee the high probability to select elite solutions, as shown in equation.
Based on the experiments, we can seethat the query cost in ImageNet is higher than that for thedataset of CIFAR-10 and CIFAR-100.
Minimum Attack Set : the minority attack methods set which could cover other attack methods completely.
With the updates of searching parameters, GAQA can adaptively work during the searching iterations and consequently quickly converge to find the near-optimal global solutions.
Adversarial example  attacks are typically launched by maliciously crafted inputs, and aim to let DNN models to misbehave.
We use Theta and Theta to denote the set of elite solutions and the set of inferior solutions in the Theta-th generation, respectively.
The identification and elimination strategy is applied to defend against potential adversarial example attacks.
In addition to the strong ability of model prediction, we assume the victim DNN models are deployed with certain defensive protections to ensure their robust to adversarial attacks.
As the primary group of machine learnings, Deep neural networks  are immense popular in regression tasks and classification tasks of emerging intelligent systems, especially for image processing systems.
Well Designed Model   on CIFAR-10: reducing capacity via depth or width at the deeper layers to improves adversarial robustness.
Different to existing works, in this paper we are interested in the generation of AEs for DNNs with defensive mechanisms.
Theta is increased after the PSO-Jump process.
In this section, we conduct the ablation experiments to test the effectiveness of our method.
However, the research to generate AEs using evolutionary algorithms is seriously overlooked for defensive DNNs.
Different to traditional GA, adversarial example-specific reproduction operators are well designed to improve search efficiency of GAQA.
We fulfil this requirement by using the probabilistic selection function, donated as equation.
As the nodes become more difficult to discriminate between their features, the downstream task models have less accuracy.
In this paper, we use GA to address the AE generation problem since it can be inherent formulated as constrained optimization problems.
It will iterativly update several edge perturbations which means SAT will search at several positions and choose the best.
To sum up, our method achieves better performance on the non-defensive models according to the experimental results of different neural network architectures, Theta norm and datasets, which demonstrates that our method is suitable for different situations.
When applying mutation on the set of perturbations, it will result in certain high-quality genes and inferior genes.
But some white-box algorithm may have different coverage compared with black-box attack.
In this paper, we aim to fast generate AEs via query-based black-box method to attack defensive DNNs deployed in MLaaS.
PSO-Jump finally will return the updated perturbations Theta to GAQA for GA-based evolution .
Section V-B.
In Section V, extensive experiments are conducted to evaluate the performance of proposed method.
Although white-box attacks have been evaluated successful, they cannot achieve high attack performance in practical applications where the information of victim models and training data are definitely not disclosed to the adversarial attackers.
We choose LTD  and FDA  as the victim neural networks.
Consequently, we define the fitness  of each perturbation solution by equation.
Since Theta is much less than Theta and Theta, time complexity of PSO-Jump can be obtained as Theta.
Theta is the best solution  of the Theta-indexed solutions for the first Theta iterations.
To facilitate the attack in practice, we presented a query-efficient black-box method, named GAQA, to generate adversarial examples.
Compared with the three black-box attack methods, our method has greater ASR and lower MQ.
While contrastive SSL methods have experienced an emergence in the past two years, such as MoCo ,  the well-established BERT  as well as the very recent MAE  in CV.
The objective of attack coverage could not be represented in the fitness function, since it depends on the experimental comparison with other attack methods.
Before the searching iteration, GAQA needs to construct the initial population.
Due to the inherent characteristics of gradient-free and adaptive query, we only need to deal with detection-based defensive mechanisms based on input transformation while ignoring the defensive mechanisms of adversarial training and gradient masking.
To improve the convergence of GAQA, we choose to upgrade the reproduction-related parameters according to the predication of searching overhead.
For example, given Theta as Theta, the expected number of chosen chromosomes can be calculated to be Theta for the population with Theta solutions.
Theta and Theta are limited to the number of population Theta.
Due to their mutual boosting effect, our method is not only prevented from falling into the gradient trap, but also speeds up the convergence of the search process.
GEMs will continually train the model to get the best parameter.
We calculate the variance of each feature dimension and multiply them by the Theta matrix to get the loss.
Due to the strong randomness of the shuffle-based-crossover, shuffle-based-crossover expands the width of the search scope instead of the depth of the scope.
The perturbation matrix for each father solution is first split into a number of square blocks.
Image compression used to reduce input space.
APGD-CE and DeepFool are white-box attack methods while ZOO, Square-Attack and GenAttack are black-box attack methods.
From the view of defense, it would be interesting to find the weakness of the deployed models using our method, and then fix the vulnerability to enhance the model robustness.
It has the high probability to increase the fitness value if changing the perturbation of  Theta  towards the direction Theta.
From another point of view, defenders could reduce the decentered neighborhood.
PSO-Jump, probabilistic-selection and block-based-crossover are used to decrease the query number, leading to a performance degradation on the non-defensive models.
Incorporating with gradient information, decision boundary-based methods are also developed, which can push the data to the decision boundary to realize misclassification of images.
The newly generated solution Theta in Theta-th iteration is related to the solution Theta and its moving speed , which is formally expressed by equation.
Some white-box attacks only search at one convex space which contains Theta.
We chose Theta and Theta norm to evaluate the effectiveness of our method on CIFAR-10 with ResNet.
Due to the explosive search space, it is extremely difficult to find the optimal AE to attack defensive DNN models.
As the future work, it is worthy of doing the following researches.
The high-quality genes from low-score chromosomes will be filtered out, whereas the high-quality genes from high-score chromosomes are mostly kept for the next searching iteration.
To be specific, SAT performs edge perturbations for those sensitive nodes.
Given a set of clean  images and the attack method A, Theta is used to denote the set of AEs which successfully attack the victim model.
The quality of population members is evaluated by a fitness function.
Given the sorted population Theta for the Theta-th iteration, the fitness value Theta of the solution Theta is the highest.
We can conclude that our GAQA is suitable to check the robustness of neural networks.
To make the attack more practical, we exploit query-based black-box method to generate image AEs.
To make AE attacks imperceptible, according to equation.
In this section, we list the basic symbol of this paper and use it to describe the work process of GEMs and following attack process.
For image classification tasks, well-designed AEs is capable of fooling DNN models with wrong predictions while being imperceptible to human naked eyes.
PSO address the optimization problem by simultaneously moving the particles in the searching space based on the corresponding fitness values.
It may be because that Genattack has no jump operator and adaptive process.
The average MCSD of GAQA is Theta, Theta, Theta, Theta and Theta	of those of APGD-CE, ZOO, Square, DeepFool, and GenAttack, respectively.
GAQA has more queries than the two white-box methods .
In contrast to white-box attack, black-box attack has a wider search space because it has no knowledge of the specific distribution of the high-dimensional space.
First, the experimental setup is given in Section V.
Probabilistic selection will only take Theta time cost.
Update adaptive parameters Theta, Theta, Theta via equation.
Perturbation-varied reproduction is designed to generate Theta new perturbations, thus it costs Theta.
To implement white-box attacks, the adversarial attackers are assumed to know the full knowledge of model architecture, model parameters and training data of the victim DNN models.
In this section, benchmark-based experiments are conducted to evaluate the efficiency of the proposed method.
Therefore, GAQA has a better attack capacity even in a poor start position, but it will take more steps to achieve the same performance.
It indicates that our method with well-designed operators has a strong searching capacity in vast space.
So we compute the sensitivity for nodes and edges separately, which is then used to update the node and edge perturbations.
After getting variance, we use Theta as an intermediate derivation variable, and use the chained derivation rule to find the sensitivity of the downstream task classifier to the variance of each feature.
Each solution could have the average perturbation range as Theta for Theta solutions of Theta.
In Section V, the detailed design of query-efficient GA attack is given.
As a prominent branch of AE attacks, a great number of white-box attacks have been developed .
In Section IV, the detailed design of query-efficient GA attack is given.
It is difficult to design an efficient algorithm that can generate adversarial examples in the discrete space.
AT is expensive to obtain holistic AEs and cannot guarantee its effectiveness facing emerging AEs, especially black-box generated AEs.
In Section III, the query-based black-box attack has been formulated as a multi-objective optimization problem, which is obviously a non-convex and discrete optimization problem.
GM is to force DNN models to have zero-close gradients, thus reducing the perturbation sensitivity to DNN models.
We use the degree of the node to estimate the sensitivity of the victim model to the attributes of the node.
